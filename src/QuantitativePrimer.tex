\documentclass[a4paper]{article}
\usepackage{qabook}


\newcommand{\docversion}{V1.0}
\begin{document}

%\doublespacing
%\linenumbers
\renewcommand{\thepage}{\roman{page}}
\setcounter{page}{0}

\begin{titlepage}
\titleDB
\end{titlepage}



\begin{center}
This work is licensed under a \\
\href{http://creativecommons.org/licenses/by-sa/4.0/}{Creative Commons Attribution-ShareAlike 4.0}\\
International License.
\end{center}


\begin{center}
You are free to:
\end{center}

\begin{description}
  \item[Share:] Copy and redistribute the material in any medium or format. In fact, you are encouraged to do so.
  \item[Adapt:] Remix, transform, and build upon the material
for any purpose, even commercially.
\end{description}

\begin{center}
Under the following terms:
\end{center}

\begin{description}
\item[Attribution:] You must give appropriate credit to the original author, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.

\item[Share Alike:] If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.
\item[No additional restrictions:] You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.
\end{description}

\begin{center}
\url{https://creativecommons.org/licenses/by-sa/4.0/legalcode}
\end{center}

\vfill
\begin{center}
\texttt{Last updated on \moddate{\jobname.tex}} \\
For errors, comments, or suggestions, contact Dirk Bester\\
\href{mailto:bester.dirkw@gmail.com}{bester.dirkw@gmail.com}\\
\href{https://www.linkedin.com/in/dwbester}{www.linkedin.com/in/dwbester}\\
Visit the GitHub page to contribute and to find the latest version
\end{center}


\clearpage

\setcounter{tocdepth}{2}
\tableofcontents

\clearpage

\renewcommand{\thepage}{\arabic{page}}
\setcounter{page}{1}

\phantomsection
\addcontentsline{toc}{section}{Introduction}
\section*{Introduction}

Interview preparation is the most important part of working life.
Relative to its duration, interviewing is the activity with the largest influence on future income.
It determines how you will be spending your time, who you will be spending it with, and what you will learn from the expenditure.
In addition to competence and merit, it takes confidence to convince the person on the other side of the table that one should be hired.
Confidence takes practise.
You can't lift 200kg without proper training.
You can't excel at interviews without befitting preparation.
The best way to improve your interviewing is to do interviews and learn from your mistakes.
The second best way to improve your interviewing is to learn from the mistakes of others, and ensure your preparation matches what you will encounter.


Quantitative finance is notorious for the large number of difficult interviews that precede an offer.
These interviews are laden with brainteasers and puzzles to test your abilities, and conveniently also serve as an intricate way to whittle down a large candidate pool.
Hopefuls who study interview technique increase their probability of traversing the interview gauntlet.
There are great books with sample questions and answers and candidates should work through most of these before attempting any interviews.
The classic text is \citet{HeardOnTheStreet}, and this is the one recruiters will suggest.
Another book is \citet{JoshiQA}.
I prefer it to the former since it has more in-depth discussions about the interviewing process, it has a whole chapter dedicated to C++ questions, and it includes references to further reading on topics encountered throughout the questions.
\citet{WilmottFAQ} supplements these by acting as a handy reference for jargon.
A suggested reading order appears in the next section.

I interviewed in London during 2014 and again in 2017,
meeting with more than 20 firms.%
\footnote{Including, but not limited to,
  Barclays,
  BlackRock,
  BNP Paribas,
  Cumulus,
  G-research,
  Goldman Sachs,
  GSA Capital,
  JP Morgan,
  MAN group,
  Oxford Asset Management,
  Royal Bank of Canada,
  Royal Bank of Scotland,
  SquarePoint Capital,
  Two Sigma,
  UBS,
and
  Winton Capital.
}
These included phone interviews, Skype interviews, online pair-programming sessions, on-site exams, take-home exams, face-to-face interviews, and multiple onsites or superdays (several interviews in one day).
The preparation books mentioned above provided great practise questions, but I feel there is more to be said on interview form.
Therefore I have prepared this text with questions I encountered, split up by interviews, to give readers an indication of exactly what to expect.
Rather than providing a large collection of questions and answers, this text focuses on the intention of the questions, a deep dive into the answers, and suggestions on communicating them during an interview.

\index{Rejection, Dealing with}
Interviewing is a numbers game.
Assume you will get one out of ten jobs you interview for and act accordingly, but don't go to any interviews without serious preparation.
Don't get too sentimental about any one team or role, and don't let rejection dismay you or take it as an indication of your ability.
Consider the following.
During my 2014 interview spate I received an offer from a hedge fund who told me I was one of the strongest candidates in their graduate pool.
A day later I received a first-round rejection from another hedge fund who derided my abilities to do basic mathematics.
Rather than pick sides, I implore the reader to accept that it is difficult for someone to judge your ability by asking you a few brainteasers or proofs---of which they already know the answer---with only ten minutes to answer it.

\phantomsection
\addcontentsline{toc}{subsection}{How to prepare}
\subsection*{How to prepare}

The minimum amount of preparation required will depend on how much you currently do and enjoy brainteasers, coding puzzles, and statistical proofs.
Of course, you'll want to do more than the minimum.
I suggest you begin with the text you are reading now, starting with a review of the calculus mentioned in appendix \ref{ap:cribsheet}.
If you can successfully do all the questions on your first try, you are probably ready.
Read through the solutions and pay attention to how the interviewers expect you to present the answers and to get an idea of how interviews progress.

If you struggle with some of the questions, work through similar questions in \citep{JoshiQA}.
Also, make sure you work through some of \citet{HeardOnTheStreet}, but allocate more time to \citet{JoshiQA}.
The former text is a classic, but it is starting to show its age---the first version was published in 1995.
The latter is more up to date and the lead author, Mark Joshi, has written plenty on quantitative-finance interviews, like
\emph{On becoming a quant}.\footnote{\url{http://www.markjoshi.com/downloads/advice.pdf}}
He is the authority when it comes to interview advice in finance, and I hope to provide an abridged version of his work.
If you feel that you have made the materials in the above your own, keep \citet{WilmottFAQ} nearby for reference and some brainteasers nastier in nature.

For general bed-time reading---as opposed to hard-core, interview-preparatory reading---I suggest
\citet{mcneil2015quantitative}, who give a thorough overview of models used for quantitative risk management, and
\citet{andersen2009handbook}, who provide an exhaustive review of modelling financial time series.
These two are tomes; you won't be able to read them cover to cover.
Don't only skim them and say you've read them.
Pick the chapter you find most palatable in each book and read it with enough interest to allow a short discussion.
\citet{narang2013inside} presents a non-technical overview of quantitative and high-frequency trading minus the hype, doing plenty to demystify the field and set the record straight.
For the hype, see
\emph{The Quants} by \citet{patterson2010quants}
or
\emph{Flash Boys} by \citet{lewis2014flash}.
Not everyone who self-identifies as a quant agrees with the contents of these, but most people in the industry have read them.
For a less flattering take, see the review of \emph{The Quants} by \citet{steinsaltz2011value} that appeared in the Notices of the American Mathematical Society.%
\footnote{\url{steinsaltz.me.uk/papers/quants_published.pdf}}

\phantomsection
\addcontentsline{toc}{subsection}{Recruiters}
\subsection*{Recruiters}
\index{Recruiters}
Few resources explain how to interact with recruiters, and the ones that do tend to be written by people who have had bad experiences.
This is certainly not representative.
In my experience the following points are important.

\emph{Don't feel bad when a recruiter goes cold on you.}
Sometimes a recruiter will stop replying your messages, especially after a failed interview.
Think about it from their perspective.
They are probably sending 10 or 20 CVs to each position.
They don't have time to get sentimental about candidates, it has to be a numbers game to them.
They have quotas to match and bosses to keep happy and thus cannot always give all of their candidates equal amounts of attention.

\emph{Don't be rude to recruiters.}
Interviewing is frustrating. Especially after several interviews with several recruiters.
Each will involve a call and a get-to-know-you chat where you have to sell yourself, for it possibly to amount to nothing.
When the frustration hits, consider this:
you have nothing to gain from being rude.
You might actually do damage to your reputation by being rude, since the market is small and recruiters talk.
You might feel used and abused, but
you have nothing to gain from being rude to a recruiter.
Even if you are not currently job hunting and a recruiter cold calls you, be courteous.
Even if you perceive unprofessional behaviour, be polite.
You have nothing to gain from being rude to a recruiter.

\emph{Don't get sentimental.}
When things don't work out for the best, the person who held the highest expectations is hurt the most.
Outcomes aren't always within one's control, but one can guard against having exaggerated expectations.
Like counting your chickens before they hatch, it is generally a bad idea to get too excited after  what felt like a successful interview.
Even if only because it can interfere with your focus.
Like the aforementioned recruiters, play the numbers game and treat every interview like just another interview---albeit an important one.
This is a difficult mindset to attain if you really need or want a job.
But it is a healthy mindset, especially when it comes negotiating.


\section{Interviews}

I made every attempt to record my encounters, but I am an unreliable narrator.
Furthermore, I only attempt a timeline of my 2017 interviews as I didn't keep a journal of interviews in 2014.
Do keep your own interview journal to help you prepare for future interviews using past experience.
Some of the firms have candidates sign non-disclosure agreements, prohibiting one from sharing the questions.
Most make no such demands.
This text respects the contractual obligations I am subject to.

\citet{HeardOnTheStreet},
\citet{JoshiQA}, and
\citet{WilmottFAQ} already exist, so I will not attempt to recreate them.
Rather, I will focus on what to answer and emphasise \emph{how} to answer it.
I would often use answers from these texts, only to find that interviewers were unsatisfied.
They would literally say ``Yes, but could you answer it again, this time in a different way.''
Not only were they looking for a textbook answer, they were looking for \emph{a specific} textbook answer.
I also give the duration of each interview.
This isn't the amount of time allocated to the technical questions, you usually have 5--15 minutes to solve each problem.
\index{Questions}

\subsection{Face-to-face, 1 hour}

\begin{question}{rolltwodice}
\index{Questions!Dice, one larger}
Roll two dice. What is the probability of one being larger than the other?
\end{question}

\begin{question}{bivariatenormalmax}
\index{Questions!E@ $E(\text{max})$ Bivariate normal}
If
\begin{align*}
  \begin{bmatrix}
  X \\ Y
  \end{bmatrix}
  \sim
  \operatorname{MultivariateNormal}
  \left(
  \begin{bmatrix}
  0 \\ 0
  \end{bmatrix}
  ,
  \begin{bmatrix}
  1      &   \rho \\
  \rho   &   1    \\
  \end{bmatrix}
  \right)
\end{align*}
find $\operatorname{E}[\max(X,Y)]$.
\end{question}

\begin{question}{makematrixfrombunchofvectors}
\index{Questions!R!Make matrix from vectors}
How would You make a matrix from a bunch of vectors in R?
\end{question}

\begin{question}{pythonlistreturnlast}
\index{Questions!Python!Return last item}
If \verb+mylist+ is a list in Python, what is \verb+mylist[-1]+?
\end{question}

\begin{question}{cppvirtualfunctionwhy}
\index{Questions!C++ virtual function}
What is a C++ virtual function, and why do we need them?
\end{question}

\clearpage
\input{answers/rolltwodice.tex}
\input{answers/bivariatenormalmax.tex}
\input{answers/makematrixfrombunchofvectors.tex}
\input{answers/pythonlistreturnlast.tex}
\input{answers/cppvirtualfunctionwhy.tex}



\clearpage
\subsection{Face-to-face, 1 hour}
\begin{question}{normalfourthmoment}
\index{Questions!Normal fourth moment}
Derive $\operatorname{E}(X^4)$ where $X \sim \text{Normal}\left(0, \sigma^2\right)$.
\end{question}


\begin{question}{arraymissingnumber}
\index{Questions!Array with missing number}
You have an unsorted array containing integers $1,2,3, \ldots, n$, but one number is missing.
Describe an algorithm to find the missing number and discuss its complexity.
\end{question}


\begin{subquestion}{arraymissingnumber:a}
What if there are $k$ missing numbers, where $k$ is much smaller than $n$.
How would you do it now?
\end{subquestion}


\begin{subquestion}{arraymissingnumber:b}
What if the initial array was sorted?
\end{subquestion}


\clearpage
\input{answers/normalfourthmoment.tex}
\input{answers/arraymissingnumber.tex}
\input{answers/arraymissingnumber_a.tex}
\input{answers/arraymissingnumber_b.tex}


\clearpage
\subsection{Face-to-face, 1 hour}
\begin{question}{normalestimatorssamplingdistribution}
\index{Questions!Normal estimators}
Suppose
$Y \sim \text{Normal}(\mu, \sigma^2)$.
Now, $10^6$ people each draw 1000 samples from this distribution.
Let $i$ denote the $i$th person.
Each person estimates the parameters of the normal distribution
$\hat{\mu}_i$
and
$\hat{\sigma}_i$
using their samples
$Y_1, Y_2, \ldots , Y_{1000}$.
How should they do this?
If you draw a histogram of the $10^6$ estimates of
$\hat{\mu}_i$ and $\hat{\sigma}_i$, what would their distributions be?
How would you prove the exact sampling distribution of $\hat{\sigma}_i$?
\end{question}


\begin{question}{derivativeofinverse}
\index{Questions!Derivative of inverse}
If we have  $g'(x)$,
what can we say about
\[
\frac{d}{dx} g^{-1}(x)
\text{?}
\]

\end{question}

\clearpage
\input{answers/normalestimatorssamplingdistribution.tex}
\input{answers/derivativeofinverse.tex}


\clearpage
\subsection{Phone interview, 45 minutes}
\begin{question}{coin100flipsgamble}
\index{Questions!Coin flip gamble}
You are presented with the following gamble.
Flip 100 fair coins.
If 60 or more are heads you win \pounds 10, otherwise you get nothing.
Should you play this game for \pounds 1?
\end{question}

\begin{question}{iszerocorrelationindependent}
\index{Questions!Correlation and dependence}
We have $X \sim \text{Normal}(0,1)$ and $Y \sim \text{Normal}(0,1)$.
If the correlation coefficient is $\rho_{XY}=0$, are $X$ and $Y$ independent?
\end{question}


\begin{question}{iszerocorrelationindependentexamples}
\index{Questions!Correlation and dependence}
Give some examples where $X$ and $Y$ are in fact dependent, but where the above still holds.
\end{question}

\clearpage
\input{answers/coin100flipsgamble.tex}
\input{answers/iszerocorrelationindependent.tex}
\input{answers/iszerocorrelationindependentexamples.tex}


\clearpage
\subsection{Online pair-programming interview, 60 minutes}
\begin{question}{twournsallocateballs}
\index{Questions!Urns and balls}
You have two urns, five red balls, and five blue balls.
You can distribute the balls into the urns any way you like, but each urn must have at least one ball in it.
I will choose one urn at random ($p=0.5$) and then draw one ball from it.
If the ball is blue, you win.
How should you distribute the balls to maximise your probability of winning?
Log into this pair-programming website and use Python or C++ to solve the problem while I watch.
\end{question}

\clearpage
\input{answers/twournsallocateballs.tex}


\clearpage
\subsection{Phone interview, 45 minutes}
\begin{question}{judgescorrectverdict}
\index{Questions!Three judges}

In a court, there are three judges.
They have the following probabilities of reaching the correct verdict.

\begin{center}
\begin{tabular}{ccc}
Judge 1 & Judge 2 & Judge 3 \\
$p$ &
$p$ &
$\nicefrac{1}{2}$ \\
\end{tabular}
\end{center}
A verdict is only decided if two or more judges agree.
What is the probability of the court reaching the correct verdict?
\end{question}


\begin{question}{regressiontheory1}
\index{Questions!Regression theory}
Some regression theory questions.
Suppose we wanted to model $Y$ using $X$, and we decided to use a linear regression:
\[
  Y = X \beta + \varepsilon
  \text{.}
\]
What assumptions are we making?
How would you find $\beta$?
What tests can we do on $\beta$ afterwards?
\end{question}


\begin{question}{bayeslawdisease}
\index{Questions!Bayes law}
Bayes law question.
Consider a particular disease.
One percent of people in the world have this disease.
The test for the disease is not perfect.
If you have the disease, the test has a 80\% chance of showing positive.
If you do not have the disease, the test has a 10\% chance of showing positive.
You go for the test and the results are positive.
What is the probability that you actually have the disease?
\end{question}

\clearpage
\input{answers/judgescorrectverdict.tex}
\input{answers/regressiontheory1.tex}
\input{answers/bayeslawdisease.tex}

\clearpage
\subsection{Onsite, 5 interviews, 3 hours}
\begin{question}{derivexpowx}
\index{Questions!Derive $x^x$}
What is $\frac{d}{dx}x^x$?
\end{question}


\begin{question}{epiandpie}
\index{Questions!E@$e$ and $\pi$}
Which is larger,
$e^\pi$
or
$\pi^e$?
\end{question}



\begin{question}{ar1processmeanandvar}
\index{Questions!AR(1) process}
We have an AR(1) process
\begin{align*}
  Y_t &= \alpha_0 + \alpha_1 Y_{t-1} + \varepsilon_{t} \\
   & \text{where} \\
   \varepsilon_{t} &\sim \text{Normal}(0,\sigma_{\varepsilon}^2)
   \text{.}
\end{align*}
What is
$\E(Y_t)$
and
$\Var(Y_t)$?
\end{question}


\begin{question}{constructzerocorrelation}
\index{Questions!Correlation and dependence}
If $X \sim \text{Normal}(0, 1)$ and $Y$ has a distribution where:
\begin{align*}
 P(Y=-1) &=  \nicefrac{1}{2} \\
 P(Y=1)  &=  \nicefrac{1}{2}
\end{align*}
What is the cumulative distribution function of $Z=XY$?
\end{question}


\begin{question}{iszerocovarianceindependent}
\index{Questions!Correlation and dependence}
If $\Cov(X,Y)=0$, are $X$ and $Y$ independent?
\end{question}


\begin{question}{stickbreak}
\index{Questions!Stick Breaking}
Break a  $1m$ stick at two random places.
What is the probability that the three resulting pieces form a triangle?
\end{question}


\begin{question}{pythonanagrams}
\index{Questions!Python!Anagrams}
Write a Python function to check whether two strings are anagrams.
Do a version with and without sorting.
Why might we want a function that can do this without sorting?
\end{question}


\begin{question}{ncralgorithm}
\index{Questions!Python!N@${}_nC_{r}$}
Without using the standard library, write a function for ${}_nC_{r}$.
Do a version with and without recursion.
\end{question}

\clearpage
\input{answers/derivexpowx.tex}
\input{answers/epiandpie.tex}
\input{answers/ar1processmeanandvar.tex}
\input{answers/constructzerocorrelation.tex}
\input{answers/iszerocovarianceindependent.tex}
\input{answers/stickbreak.tex}
\input{answers/pythonanagrams.tex}
\input{answers/ncralgorithm.tex}

\clearpage
\subsection{Phone interview,  50 minutes}
\begin{question}{romeojuliet}
\index{Questions!Romeo and Juliet}
Romeo and Juliet agree to meet between 08:00 and 09:00.
Each arrives at a random time in the hour and then waits 15 minutes.
What is the probability that they meet?
\end{question}

\clearpage
\input{answers/romeojuliet.tex}


\clearpage
\subsection{Onsite, 6 interviews, 6 hours}
\begin{question}{davidbeckhamparty}
\index{Questions!David Beckham party}
Consider a party where there are $N$ people present.
We have a function that tests whether person $a$ knows person $b$:
\[
  \text{knows}(a, b)
  \text{.}
\]
The function returns true or false.
It is not necessarily symmetric,
\[
  \text{knows}(a, b) \neq \text{knows}(b, a)
  \text{.}
\]
For instance, I know David Beckham, but he doesn't know me.
At a specific party, every guest knows at least one other person.
Except for David Beckham, who is also at the party.
He is special in that everyone knows him, but he knows no one at this particular party.
Numbering the people at the party from $1$ to $N$ and using the
$\text{knows}()$
function, how would you determine which number is David Beckham?
Now pretend Victoria Beckham is also at the party and that she only knows David, and he only knows her (and everyone at the party knows both of them and at least one other person).
How would you find them?

\end{question}



\begin{question}{arrayofintegersfindduplicate}
\index{Questions!Array find duplicate}
\index{Questions!Sorting}
We have an array of $N$ integers, unsorted:
\begin{align*}
  [n_1, n_2, \ldots , n_{N} ]
  \text{.}
\end{align*}
All the integers are unique, except two.
These are $N$ arbitrary integers; they aren't necessarily the numbers from $1$ to $N$.
How would you find the duplicate?
Give an answer that doesn't rely on sorting.
Give an answer with sorting, and discuss your favourite sorting algorithm.
\end{question}


\begin{question}{criminalsinfield}
\index{Questions!Criminals in a field}
You have 100 criminals in a field and you have a gun with one bullet.
The criminals are brave, if any one of them has a non-zero probability of surviving he will attempt an escape.
But they are not suicidal and will not run if they are certain of death.
How would you stop them from escaping?
\end{question}

\clearpage
\input{answers/davidbeckhamparty.tex}
\input{answers/arrayofintegersfindduplicate.tex}
\input{answers/criminalsinfield.tex}


\clearpage
\subsection{Phone interview, 45 minutes}
\begin{question}{regressiontheory2}
\index{Questions!Regression theory}
Questions on logistic regression and the test you perform on the significance of its parameters (vs the test used for linear regression).
\end{question}


\begin{question}{twopiecesofwood}
\index{Questions!Measure pieces of wood}
I have two pieces of wood of length $a$ and $b$, where $a<b$.
I have a measuring apparatus with a variance of $\sigma^2$ (due to measurement error).
I can only use it twice.
What is the best way to measure $a$ and $b$?
\end{question}



\begin{question}{bagnsockstwored}
\index{Questions!Bag of socks}
I have a bag with $N$ socks.
Some of them black, some of them red.
If I randomly pick two socks,
the probability that they are both red is $\nicefrac{1}{2}$.
What is the smallest possible value of $N$ for which this is possible?
\end{question}

\clearpage
\input{answers/regressiontheory2.tex}
\input{answers/twopiecesofwood.tex}
\input{answers/bagnsockstwored.tex}


\clearpage
\subsection{Onsite, 5 interviews, 3 hours}
\begin{question}{regressiontheory3}
\index{Questions!Regression theory}
Linear regression theory.
Assumptions?
What happens if we have multicollinearity?
How would you measure goodness of fit?
\end{question}


\begin{question}{nameafewnonlinearmodels}
\index{Questions!Non-linear models}
Name a few non-linear models.
\end{question}



\begin{questionwithnoanswer}
\index{Questions!SQL}
Write some SQL queries.
Consider a table with the following headings.
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
   Employee name & Department & Salary \\
\hline
\end{tabular}
\end{center}
\end{questionwithnoanswer}

\begin{subquestion}{sqladdaveragesalary}
Add a column to show the average dept salary for each employee.
\end{subquestion}


\begin{subquestion}{sqlhighestsalary}
Write a query to return the second highest salary.
\end{subquestion}



\begin{question}{sqlfindcustomerswithchanges}
You are given the following table.
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
     Customer ID    &   Name     &  datestart     & dateend     \\
\hline
     A              &   Jon      &  1991-05-06    & 1992-05-06  \\
     A              &   Jonathan &  1992-05-06    & 1998-10-02  \\
     B              &   Chris    &  1983-01-01    & 1997-12-16  \\
     C              &   Sean     &  1991-05-06    & 2000-05-12  \\
\hline
\end{tabular}
\end{center}
Write a query to return all the customers that have made changes to their details.
\end{question}

\clearpage
\input{answers/regressiontheory3.tex}
\input{answers/nameafewnonlinearmodels.tex}
\input{answers/sqladdaveragesalary.tex}
\input{answers/sqlhighestsalary.tex}
\input{answers/sqlfindcustomerswithchanges.tex}


\clearpage
\subsection{Face-to-face, 2 hours}
\begin{question}{regressiontheory4}
\index{Questions!Regression theory}
Questions about linear regression and logistic regression, specifically about the assumptions and significance tests.
\end{question}


\begin{questionwithnoanswer}
A series of questions about R.
I got these questions since I indicated R to be my language of choice.
If you display a preference for Python, adjust the questions accordingly.
\end{questionwithnoanswer}

\begin{subquestion}{rdatastructures}
\index{Questions!R!Data structures}
What are some data structures used in R?
\end{subquestion}



\begin{subquestion}{rmergetwodataframes}
\index{Questions!R!Merge data frames}
Describe the syntax you would use to merge two data frames.
\end{subquestion}



\begin{subquestion}{rapplyvsforloops}
\index{Questions!R!L@\verb+lapply+ vs for loop}
What is the difference between \verb+lapply()+ and for loops?
Which do you prefer?
\end{subquestion}


\begin{question}{grownup}
Received a few case-study questions about problems the team was currently facing.
\end{question}

 \clearpage
\input{answers/regressiontheory4.tex}
\input{answers/rdatastructures.tex}
\input{answers/rmergetwodataframes.tex}
\input{answers/rapplyvsforloops.tex}
\input{answers/grownup.tex}

\clearpage
\subsection{Video interview, 1 hour}
\begin{question}{simplerandomwalkhittingprobability}
\index{Questions!Simple random walk hitting probability}
You have a simple random walk,
\[
  X_t = \sum_{i=1}^{t}{Z_i}
\]
where
\begin{align*}
 P(Z_i = 1) &= 0.5 \\
 P(Z_i = 0) &= 0.5
 \text{.}
\end{align*}
If the process is currently at $k$, meaning $X_0=k$, what is the probability that it would hit $l$ before hitting $0$,
where $k < l$?
\end{question}

\clearpage
\input{answers/simplerandomwalkhittingprobability.tex}

\clearpage
\section{Old friends}
Two questions stood out.
I got the \emph{Air Force One} question three times in 2014, and I got the \emph{Stick Breaking} question more than three times during 2017.\footnote{I never got \emph{Air Force One} in 2017, nor did I get \emph{Stick Breaking} in 2014. Questions seem to go in and out of fashion and there is evidence of cross-pollination.}
Getting a question multiple times allows you to repeatedly sample the experience, and that way you learn more about the uncertainty in the demeanour of interviewers.\footnote{See question \ref{q:twopiecesofwood}.}
Like stand-up comedians who find that different crowds laugh at different parts of the same joke, different interviewers didn't always approve of the same answer.
Some are looking for an intuitive answer  while others want you to grind through the mathematics, rigorously proving every step.
While both of these questions appear in the popular interview manuals---both with elegant solutions---I can report that these answers aren't sufficient to satisfy interviewers.
Thus, I recommend a deeper dive.
For each of these questions I will present the answers from the manuals and then show how my interviewers wanted them to be answered.


\subsection{Air Force One}
\label{q:airforceone}
\index{Questions!Air Force One}
This question is sometimes called ``the drunken passenger''.
\begin{quote}
One hundred people are in line to board a plane.
There are exactly 100 seats on the plane.
Each passenger has a ticket. Each ticket assigns the passenger to a specific seat.
The passengers board the aircraft one at a time.
The first person to board the plane is drunk and does not pay attention to his ticket.
He picks a seat at random and pretends that it is his proper seat.
The remaining passengers board the plane one at a time. If one of them finds their assigned seat empty, they will sit in it.
If they find that their seat is already taken, they will pick a seat at random.
This continues until everyone has boarded the plane and taken a seat.
What is the probability that the last person to board the plane sits in their proper seat?
\end{quote}
This was the first brainteaser I was ever asked in a live interview.
It is question 3.25 in \citet{JoshiQA}, and the question titled ``Air Force One'' in \citet{WilmottFAQ}.
Let's look at their solutions, reproduced verbatim.
\citet{JoshiQA}:
\begin{quote}
There are a number of ways to do this. Most of
these are tricky and complicated algebraically. However, if we condition on the
right event it becomes easier.

Every person who does not sit in their own seat can do three things. Sit in
seat 1, sit in seat 100 or force someone else to sit in the wrong seat.

If they sit in seat 1, then every subsequent person sits in the correct seat,
and therefore so does the last passenger.

If they sit in seat 100, then the last passenger sits in the wrong seat.

If they sit in any other seat, we are back in the same situation with fewer
passengers and seats.

Consider the last passenger who has a choice. This is a passenger below 100
who has been displaced. Since he is the last one displaced he must either sit in
seat 100 or seat 1.

The answer is therefore 1/2, since the probability of sitting in either of these
two seats is equal.
\end{quote}
Simple! You think nothing more of it and give this answer when prompted. Then the interviewer says ``I don't understand your answer, could you give a different one?''
Even though you trust and believe the simple solution, chances are that the interviewer wants to see \emph{some} mathematics.
The first two times I got this question I got stuck after giving a simple answer and not knowing how to convince the interviewer that I understood the principles behind it.
Before we do the algebra, let's see what \citet{WilmottFAQ} has to say. (Note that he calls the confused passenger GW, who is about to board Air Force One, but cannot read and therefore ignores his ticket.)
\begin{quote}
Sounds really complicated, because of all the people
who could have sat in the last person’s seat before
their turn. Start by considering just two people, GW and
you. If GW sits in his own seat, which he will do 50\%
of the time, then you are certain to get your allocated
seat. But if he sits in your seat, again with 50\% chance,
then you are certain to not get the right seat. So a priori
result, 50\% chance. Now if there are three people, GW
either sits in his own seat or in your seat or in the
other person’s seat. The chances of him sitting in his
own seat or your seat are the same, and in the former
case you are certain to get your correct seat and in
the latter you are certain to not get it. So those two
balance out. If he sits in the other person’s seat then
it all hinges on whether the other person then sits in
GW’s seat or yours. Both equally likely, end result 50-50
again. You can build on this by induction to get to the
simple result that it is 50-50 whether or not you sit in
your allocated seat.
\end{quote}
This answer is a bit more informative, and Wilmott left the algebra as an exercise to the reader.
Let us do it here.
His suggestion to start with the case of one passenger is a good idea, and it is the first trick we discuss in appendix \ref{ap:tricks}.
The notation can be tricky, so it is good to work through this beforehand so you don't struggle in a live interview.
Note that any person who gets on the plane and finds his seat occupied \emph{becomes the drunkard.}
This means that if there are 100 seats, and the drunk passenger sits in seat 50, people 2--49 will just sit in their assigned seats.
When person 50 finds his seat occupied, he becomes the drunkard and we now have the same problem as before, but with 51 people instead of 100. The open seats will be
$1, 51,52,\ldots,99,100$
and passenger 50 can choose any of them randomly. For simplicity, we can relabel seat $1$ and call it seat $50$, and then we pretend it is the allocated seat for passenger 50 (since it is the ``correct'' choice that makes everyone else get their assigned seats).
This allows us to solve the problem using induction.

Let $p_n$ be the probability that the last person sits in their seat when there are $n$ seats on the plane.
We want to determine $p_{100}$, since it is what the question requires.
Let $A$ be the event that the last person sits in their own seat, and $B_i$ be the seat number chosen by a person who gets on when there are $i$ seats left.
It has a discrete uniform distribution from $1$ to $i$, so $P(B_i = k) = \frac{1}{i}$ since there are $i$ seats and the person randomly chooses one.
We start with the trivial case of $n=1$ where
\[
  p_1 = 1
  \text{,}
\]
which doesn't help.
To keep the notation manageable, assume the $n$th person is assigned to seat $n$.
Now consider the case with two people,
\begin{align*}
  p_2 = P(A|B_2 = 1)P(B_2=1) + P(A|B_2 = 2)P(B_2=2)
  \text{.}
\end{align*}
Here, think of  $P(A|B_2 = 1)$ as ``the probability that the last person sits in his own seat if person just before him sits in their own seat.''
Similarly, think of  $P(A|B_2 = 2)$ as ``the probability that the last person sits in his own seat if person just before him actually took it by mistake'', which is zero.
\begin{align*}
  p_2 &= (1)\frac{1}{2} + (0)\frac{1}{2} \\
  &= \frac{1}{2}
\end{align*}
That's a lot of notation to arrive at something trivial.
The picture starts to emerge when we look at $n=3$.
It helps to think about it in words first.
When there are three people, the first one to board the plane can do one of three things, as highlighted in the following table.
\begin{center}
\begin{tabular}{lclc}
\hline
 Action                    & $P(\text{Action})$ &  Consequence                             & $P(A|\text{Action})$ \\
                           & $P(B_3 = k)$       &                                          &               \\
 \hline
He sits in seat 1 (his own)& $\nicefrac{1}{3}$  &  Crises averted                          & 1\\
He sits in seat 2          & $\nicefrac{1}{3}$  &  The second person becomes the drunkard  & $p_2$\\
He sits in seat 3          & $\nicefrac{1}{3}$  &  The last person is doomed               & 0\\
\hline
\end{tabular}
\end{center}
Recall that $A$ is the event where the last person to board the plane gets their allocated seat. Let's express this in our confusing notation:
\begin{align*}
  p_3
  &=
       \phantom{{} + {}}    P(A|B_3 = 1)P(B_3=1) \\
      &\phantom{{} = {}} +  P(A|B_3 = 2)P(B_3=2) \\
      &\phantom{{} = {}} +  P(A|B_3 = 3)P(B_3=3)
  \\
  &=
  (1)   \frac{1}{3}  +
  (p_2) \frac{1}{3} +
  (0)   \frac{1}{3} \\
& =\frac{1}{3}\left(1 +  \frac{1}{2} + 0\right) \\
& =\frac{1}{3}\left(\frac{3}{2}\right)  \\
& =\frac{1}{2}
\text{.}
\end{align*}
Doing it for $n=4$ reveals the pattern.
The first person to board the plane has the following choices.
\begin{center}
\begin{tabular}{lclc}
\hline
 Action                    & $P(\text{Action})$ &  Consequence                             & $P(A|\text{Action})$ \\
                           & $P(B_4 = k)$       &                                          &               \\
 \hline
He sits in seat 1 (his own)& $\nicefrac{1}{4}$  &  Crises averted                          & 1\\
He sits in seat 2          & $\nicefrac{1}{4}$  &  The second person becomes the drunkard  & $p_3$\\
He sits in seat 3          & $\nicefrac{1}{4}$  &  The third person becomes the drunkard   & $p_2$\\
He sits in seat 4          & $\nicefrac{1}{4}$  &  The last person is doomed               & 0\\
\hline
\end{tabular}
\end{center}
\begin{align*}
  p_4
      &=  \phantom{{} + {}} P(A|B_4 = 1)P(B_4=1) \\
      &\phantom{{} = {}} +  P(A|B_4 = 2)P(B_4=2) \\
      &\phantom{{} = {}} +  P(A|B_4 = 3)P(B_4=3) \\
      &\phantom{{} = {}} +  P(A|B_4 = 4)P(B_4=4)
  \\
 & =
  (1)   \frac{1}{4} +
  (p_3) \frac{1}{4} +
  (p_2) \frac{1}{4} +
  (0)   \frac{1}{4} \\
& =\frac{1}{4}\left(1 +  \frac{1}{2} + \frac{1}{2} + 0\right) \\
& =\frac{1}{4}\left(\frac{4}{2}\right)  \\
& =\frac{1}{2}
\text{.}
\end{align*}
Doing it for $n$ people, the first person to board the plane can choose from the following actions.
\begin{center}
\begin{tabular}{lclc}
\hline
 Action                    & $P(\text{Action})$ &  Consequence                             & $P(A|\text{Action})$ \\
                           & $P(B_n = k)$       &                                          &               \\
 \hline
He sits in seat 1 (his own)& $\nicefrac{1}{n}$  &  Crises averted                          & 1\\
He sits in seat 2          & $\nicefrac{1}{n}$  &  The second person becomes the drunkard  & $p_{n-1}$\\
He sits in seat 3          & $\nicefrac{1}{n}$  &  The third person becomes the drunkard   & $p_{n-2}$\\
He sits in seat 4          & $\nicefrac{1}{n}$  &  The fourth person becomes the drunkard  & $p_{n-3}$\\
\vdots                     &   \vdots           &   \vdots                                 & \vdots \\
He sits in seat $i$        & $\nicefrac{1}{n}$  &  The $i$th person becomes the drunkard   & $p_{n-i+1}$\\
\vdots                     &   \vdots           &   \vdots                                 & \vdots \\
He sits in seat $n-3$      & $\nicefrac{1}{n}$  &  The third-last person becomes the drunkard & $p_{4}$\\
He sits in seat $n-2$      & $\nicefrac{1}{n}$  &  The third-last person becomes the drunkard & $p_{3}$\\
He sits in seat $n-1$      & $\nicefrac{1}{n}$  &  The second-last person becomes the drunkard & $p_{2}$\\
He sits in seat $n  $      & $\nicefrac{1}{n}$  &  The last person is doomed               & 0\\
\hline
%                           &                    &                                          & \\
\end{tabular}
\end{center}
\begin{align*}
  p_n
  &=
   \phantom{{} + {}}     P(A|B_n = 1)P(B_n=1) \\
  &\phantom{{} = {}} + P(A|B_n = 2)P(B_n=2) \\
  &\phantom{{} = {}} + P(A|B_n = 3)P(B_n=3) \\
  &\phantom{{} = {}}\phantom{{} + {}} \vdots  \\
  &\phantom{{} = {}} +  P(A|B_n = i)P(B_n=i) \\
  &\phantom{{} = {}}\phantom{{} + {}} \vdots  \\
  &\phantom{{} = {}} +  P(A|B_n = n)P(B_n=n) \\
&= (1)       \frac{1}{n}  +
   (p_{n-1}) \frac{1}{n}  +
   (p_{n-2}) \frac{1}{n}  +
  \ldots +
  (p_{n-i+1})\frac{1}{n} +
  \ldots +
  (0)        \frac{1}{n}  \\
&=\frac{1}{n}\left(1 +  \frac{1}{2} + \frac{1}{2} + \ldots + \frac{1}{2} + \ldots + 0\right) \\
&=\frac{1}{n}\left(\frac{2}{2} + \frac{n-2}{2} + 0\right) \\
&=\frac{1}{n}\left(\frac{n}{2}\right)  \\
&=\frac{1}{2}
\text{.}
\end{align*}
We see that for any $n$, the answer is $\nicefrac{1}{2}$.
If you want more rigour you could do this proof by mathematical induction.
During an interview it is easy to get confused with the notation.
The one I used here is not necessarily the easiest and I suggest avoiding too much notation altogether by rather drawing the tables as above to show your thinking.
It should be enough to satisfy an interviewer.


\subsection{Stick Breaking}
\label{subsec:stickbreaking}
\index{Questions!Stick Breaking|textbf}
The question is usually presented as follows.
\begin{quote}
Break a 1 meter stick randomly in two places. What is the probability that the three resulting pieces form a triangle?
\end{quote}
This is question 3.27 from \citet{JoshiQA}, but they present it with less ambiguity:
\begin{quote}
Let $x$,$y$ be uniformly distributed on $[0, 1]$ and separate the
unit interval $[0,1]$ into three pieces, what is the probability that the three pieces of the
line can be constructed into a triangle?
\end{quote}
The difference is subtle; with the first wording we might interpret it as breaking the stick once, randomly selecting one of the pieces, and breaking that one again.
It is a completely different question, but most interviewers won't be aware of this ambiguity.
Just tell the interviewer you assume that two uniform random numbers are chosen before breaking the stick and you should be fine.
Joshi gives the following answer, reproduced verbatim.
\begin{quotation}
First, we need to transform the condition into something less opaque.
The three pieces will form a triangle if the longest piece is smaller in size than the sum of the other two.
This will happen if and only if the biggest piece is of size less than a half.
Unfortunately, we do not know at the start which piece will be longest.
However, this happens if precisely one of $x$ and $y$ is less than $\frac{1}{2}$, and $|x-y|<\frac{1}{2}$ (If both were in the same half interval, the longest piece would contain the other half interval.)

As usual when drawing two points, it is best to think of $(x,y)$ as a point in the unit square.
We work out the geometry of the valid set in order to compute its area.
The condition that precisely one is less than $\frac{1}{2}$, means that if we divide into  four squares of size $\frac{1}{2}$ we are in the top left or the bottom right.
Our remaining constraint says that we are below the diagonal from bottom half to top right in
the top left square, and above the same diagonal in the bottom right square.

The set therefore consists of two half small squares, and is of area equal to one small square.
The answer is therefore $\frac{1}{4}$.
\end{quotation}
There is a lot to unpack here.
The first thing to convince yourself (and the interviewer) of is that a triangle can only be formed if the largest piece is less than the sum of the other two.
It is best to draw pictures of the three possible cases.
\begin{enumerate}
  \item When the longest piece is less than the sum of the other two
  \item When the longest piece is equal to the sum of the other two
  \item When the longest piece is greater than the sum of the other two
\end{enumerate}
You will find that only the first case allows you to draw a triangle.
The second case is a degenerate case where the triangle you draw becomes a line, and the third case doesn't allow for a triangle to be formed.
The next part of their answer is a bit of brilliant intuition.
He correctly realised that the largest piece will be less that the other two if and only if precisely one of $x$ and $y$ is less than $\frac{1}{2}$, and $|x-y|<\frac{1}{2}$.
Thereafter, he uses the trick to integrate a bivariate uniform distribution by using the unit square (see appendix \ref{ap:tricks}).
If you think about it, it makes sense, but it is difficult to arrive at this condition by yourself.
It is even harder to convince an interviewer of the correctness of this condition if they are expecting the long answer.
So, as ever, let us consider the long answer.

Use the notation from \citet{JoshiQA}, and let $x$ and $y$ be two random points on the interval $[0,1]$.
We break the stick at $x$ and $y$ and then define the three resulting lengths as
$l_1$,
$l_2$, and
$l_3$.

\begin{center}
\begin{tikzpicture}
\draw[decorate, decoration={brace, amplitude=10pt}]
      (0,1) -- (12,1) node (A) [midway, yshift=20pt]{1m};
\filldraw
(0,0)
node[] (start) {}
--
(4.5,0)
circle (2pt)
node[below, yshift=-0.5ex]  {$\min(x,y)$}
--
(8,0)
circle (2pt)
node[below, yshift=-0.5ex] (y) {$\max(x,y)$} --
(12,0)
node[align=right,  below]
(end) {};
\draw[decorate, decoration={brace, mirror ,  amplitude=13pt}]
(0.1,-1) -- (4.4,-1)  node (A) [midway, align=center,  yshift=-20pt]{\small $l_1 = \min(x,y)$};
\draw[decorate, decoration={brace, mirror ,  amplitude=13pt}]
(4.6,-1) -- (7.9,-1)  node (A) [midway, yshift=-20pt]{\small $l_2 = \max(x,y)-\min(x,y)$};
\draw[decorate, decoration={brace, mirror ,  amplitude=13pt}]
(8.1,-1) -- (11.9,-1)  node (A) [midway, yshift=-20pt]{\small $l_3 = 1-\max(x,y)$};
\end{tikzpicture}
\end{center}
We know that the triangle can only be formed when the largest piece is less than the sum of the other two, but we have to express this as something we can use algebraically.
Let
$l_\text{min}$,
$l_\text{mid}$, and
$l_\text{max}$
be the minimum, middle, and maximum values of
$(l_1, l_2, l_3)$.
Then
\begin{align*}
  l_\text{max} < l_\text{min} + l_\text{mid}
\end{align*}
and since
$  l_\text{min} + l_\text{mid} + l_\text{max}  = 1 $,
we have
\begin{align*}
  l_\text{max} < 1 - l_\text{max}  \\
2 l_\text{max} < 1 \\
 l_\text{max} < \frac{1}{2}
\end{align*}
This is a necessary condition, and if
$\max(l_1, l_2, l_3)$
has to be less than
$\nicefrac{1}{2}$
then all three of
$l_1$, $l_2$, and $l_3$
must be less than $\nicefrac{1}{2}$.
It is the crux of the question.
We have to find
\begin{align*}
&\phantom{=}
P\left(
l_1 < \frac{1}{2} ,
l_2 < \frac{1}{2} ,
l_3 < \frac{1}{2}
\right)
\text{.}
\end{align*}
To evaluate this probability, we will integrate on the unit square to find the area where our crucial condition holds for any $(x,y)$ pair.
It is easier to consider the cases where $x<y$ and $x \geq y$ separately.
When $x<y$:
\begin{align*}
x     &< \frac{1}{2} \\
   y - x   &< \frac{1}{2} \\
1 -   y    &< \frac{1}{2}
\end{align*}
and we can find the area on the unit square where this holds.
\index{Tricks!Unit square integration}
Write these three conditions in the form $y < mx + c$,
\begin{align}
x  &< \frac{1}{2}     \label{eq:stickbreaking:line1}  \\
y  &< \frac{1}{2} + x \label{eq:stickbreaking:line2}  \\
y  &> \frac{1}{2}     \label{eq:stickbreaking:line3}
\text{,}
\end{align}
which we can plot as lines on the unit square.
\begin{center}
\begin{tikzpicture}[scale=6, domain=0:1]
\draw (0,0)     --  (0,1) node[midway, left] {$y$}-- (1,1)  -- (1,0) -- (0,0) node[midway, below] {$x$};
\filldraw[pattern=dots, pattern color=lightgray] (0.5,0.5) -- (0.5,1) -- (0.0,0.5) -- (0.5,0.5);
\draw (0.5,0.5) -- (0.5,1) node[midway, right] {(\ref{eq:stickbreaking:line2})};
\draw (0,0.5)   -- (0.5,1) node[midway, above, sloped] {(\ref{eq:stickbreaking:line1})};
\draw (0.0,0.5) -- (0.5,0.5) node[midway, below] {(\ref{eq:stickbreaking:line3})};
\draw (0,0)     -- (1,1) node[midway, below, sloped]  {$y=x$} ;

\begin{scope}[xshift=1.4cm, yshift=-0.2cm, every node]

\draw[decorate, decoration={brace,  amplitude=13pt}] (0,1.08) -- (0.5,1.08) node [midway, yshift=+22pt]{$\frac{1}{2}$};
\draw[decorate, decoration={brace,  amplitude=13pt}] (-0.03,0.55) -- (-0.03,1.05) node [midway, xshift=-20pt]{$\frac{1}{2}$};

\filldraw[pattern=dots, pattern color=lightgray] (0.5,0.5) -- (0.5,1) -- (0.0,0.5) -- (0.5,0.5);
\filldraw[pattern=dots, pattern color=lightgray] (0,1.05) -- (0.5,1.05) -- (0.0,0.55) -- (0,1.05);

\end{scope}
\end{tikzpicture}
\end{center}
The area where all of our conditions hold is the grey triangle.
When $y \leq x$, the region of interest will be mirrored in the line $y=x$.
Our answer becomes the area of the square made up by the two triangles shown on the right, which is
$\frac{1}{2} \times \frac{1}{2} = \frac{1}{4}$, or exactly a quarter of the unit square.

This square is what Joshi described in his answer.
When you need to integrate a bivariate uniform distribution on $[0,1]$, don't.
Rather, follow the advice in appendix \ref{ap:tricks} and draw a unit square and construct a shaded region therein.
For this questions, I would suggest to know the method from \citet{JoshiQA}, in case the interviewer is looking for that answer, but be ready to work through it step by step as well.


% This line sets the tocdepth from here on forward
% Default is 2
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}

\section{Regression theory}
\label{sec:regressiontheory}
\index{Regression theory}
The classic interview texts do not provide much information on regressions, but regression questions have become more prevalent in recent years.
The first part in this section deals with multiple linear regression (the case where there are multiple explanatory variables). It is a generalisation of simple linear regression (only one explanatory variable) which is discussed thereafter.
For simple linear regression there are a few additional results that come in handy during interviews and it is worth looking over those.
After these two, I also provide some notes about the logistic regression.
Linear and logistic regression make up the most prevalent models encountered not only in interviews, but also in practice.
In all cases, the Wikipedia articles are great overviews and I recommend reading them before interviews, in addition to this chapter.\\
\begin{itemize}
  \item \url{https://en.wikipedia.org/wiki/Linear_regression}
  \item \url{https://en.wikipedia.org/wiki/Simple_linear_regression}
  \item \url{https://en.wikipedia.org/wiki/Logistic_regression}
\end{itemize}

\subsection{Linear regression}
\index{Regression theory!Linear}
Also known as multiple linear regression, to indicate that it uses multiple covariates.
Let $\beta$ be a matrix of parameters, and let $X$ be a vector of covariates.
We make the following assumption for each $Y$ we observe:
\[
  Y = X \beta + \varepsilon
  \text{.}
\]
We will refer to $Y$ as the variate, to $X$ as the covariates, and $\beta$ as the parameters.


\subsubsection{Assumptions}
\begin{description}
  \item[Weak exogeneity.] The covariates are observed without error.
  \item[Linearity.] The mean of the variate is a linear combination of the parameters and the covariates.
  \item[Constant variance.] Call it homoscedasticity for bonus points on terminology (to be contrasted with heteroscedasticity).
  This means all the observations of $Y$ are assumed to have the same, constant variance.
  \item[Independence of errors.] We assume that the errors of the variates are uncorrelated.
  \item[No multicollinearity.] Or rather, the lack of perfect multicollinearity. We assume that the covariates aren't perfectly correlated. See discussion below.
  \item[Errors have a statistical distribution (Optional).] This isn't strictly necessary, but it makes prediction theoretically coherent. The usual assumption is that the errors have a normal distribution, but others can also be used.
  (For instance, the t-distribution is used for ``robust regression'', where the variance observed is larger than that of the normal distribution.)
\end{description}



\subsubsection{If assumptions don't hold}


\begin{description}
  \item[Weak exogeneity.] We can test the sensitivity of our model to the assumption of weak exogeneity by doing bootstrap sampling for the covariates and seeing how it affects the parameter estimates.
  Covariates measured with error used to be a difficult problem to solve, as it requires errors-in-variables models, which have very complicated likelihoods. In addition, there is no universal fitting library to deal with these. But nowadays, with the availability of Markov Chain Monte Carlo (MCMC) estimation through probabilistic programming languages, it is a lot easier to deal with these using Bayesian hierarchical models (or multilevel models, or Bayesian graphical models---these have many names).
  \item[Linearity.] The linear regression model only assumes linearity in the parameters, not the covariates. Therefore  we could build a regression using non-linear transformations of the covariates, for instance,
  \[
    Y = X_1 \beta_1 +
        X_1^2 \beta_2 +
        \log(X_1) \beta_3
    \text{.}
  \]
  If we need to further relax the assumption, we are better off using non-linear modelling,
  see answer \ref{a:nameafewnonlinearmodels} for some examples.
  \item[Constant variance.] There are some avenues we can take here. The simplest is to do a variance-stabilising formula on the data. We could also assume a constant coefficient of variation, rather than a constant mean. Some estimation libraries (such as the \verb+glm+ package in R) allows specifying the variance as a function of the mean.
  \item[Independence of errors.] This is a dangerous one. In the financial world things are often highly correlated in times of crisis. The first thing you should do is understand how risky this assumption is for your setting. If necessary, add a correlation structure to your model, or do  a multivariate regression. Both of these require significant resources to estimate parameters, not only in terms of computational power but also in the amount of data required.
  \item[No multicollinearity.] If our covariates are correlated, we can still fit our regression, but we might run into numerical problems depending on how our fitting algorithms invert the matrices involved.
  We can also no longer trust the t-tests that our regression produces; we have to include all the covariates regardless of what their significance tests say.
  A big problem with multicollinearity, however, is over fitting.
  Depending on how bad the situation is, the parameter values might have huge uncertainties around them, and if you fit the model using new data their values might change significantly.
  I suggest reading the Wikipedia article on multicollinearity, as it contains useful information. \\
  \url{https://en.wikipedia.org/wiki/Multicollinearity} \\
  Multicollinearity is a favourite topic of discussion for interviewers, and they usually have strong opinions about how it should be handled.
  I will not impart my own opinions, lest the reader gets penalised for them.
  Just make sure you \emph{do} have an opinion on this matter, and that it is an informed one.
  \item[Errors have a statistical distribution (Optional).] The errors will \emph{always} have a distribution, whether you account for it or not (your humble author is extremely prejudiced against methods that merely fit the best line through some data without thinking about the statistical distribution lurking behind it).
  The model's intended use will determine how sensitive it is to ignoring the error distribution.
  In many cases, fitting a line using least-squares estimation is equivalent to assuming errors have a normal distribution.
  If the real distribution has heavier tails, like the t-distribution, how risky will it make decisions based on your outputs?
  One way to address this is to use a technique like robust-regression.
  Another way is to think about the dynamics behind the problem and which distribution would be best suited to model them---as opposed to just fitting a curve through a set of points.
\end{description}

\subsubsection{Significance tests}
In the ordinary linear regression, the t-statistic is used to test the significance of each parameter,
\[
t_{\hat\beta}  =  \frac{\hat\beta - \beta_0}{ \text{stdev}( \tilde\beta) }
\]
where
${\text{stdev}( \tilde\beta)}$
is the standard deviation of the estimator
$\tilde\beta$.
For the classical linear regression with homoscedasticity and normal errors,
the sampling distribution of
$t_{\hat\beta}$
is the student's t-distribution with $(n-p)$ degrees of freedom.
Here, $\beta_0$ is a hypothesised value to test $\hat\beta$ against, usually set to $0$.

When multicollinearity is present, we should not place too much emphasis on the interpretation of these statistics, as the  $\text{stdev}( \tilde\beta) $ will be high and the $\tilde\beta$ will be correlated. The t-statistic should not be confused with the Wald statistic, which is the equivalent test used in the case of logistic regression as we will see below.

\subsubsection{Parameter estimation}
Since linear regressions are so well studied and prevalent in the literature, there exists a multitude of ways to estimate the parameters.
They can largely be grouped into the following:
\begin{description}
  \item[Least squares and related techniques.] This gives the well-known result
  \[
    \hat{\beta} = (X^T X)^{-1}X^T y
    \text{.}
  \]
  But inverting the matrix might be numerically troublesome in the case of high multicollinearity.
  Ordinary least squares implies the assumption of the normal distribution, and if we believe there is more variance in our data than the normal distribution can explain, we are better off using robust least-squares or likelihood-based techniques.
  \item[Techniques based on the likelihood.]
  This necessitates assuming a statistical distribution on the data.
  To the frequentist, it then means maximising the likelihood.
  To the Bayesian, it means assuming a prior, multiplying it by the likelihood, and using the arising posterior distribution for inference.
\end{description}
Make sure you know some theory behind your favourite method and also know how to fit the linear regression using your favourite library without needing to Google.
Even better, know how to do this using both R and Python.
The section below provides a quick derivation of the least-squares result, followed by a section containing the derivation of the maximum likelihood estimate.


\subsubsection{Least squares}
\index{Least squares}
\index{OLS|see {Least squares}}
Assume we have $n$ rows in our dataset, and $p$ covariates.
We then present the model in vectorised form
\begin{align*}
Y = X \beta + \varepsilon
\text{,}
\end{align*}
where $\beta$ is a $(p \times 1)$ vector,
$X$ is an $n \times p$ matrix, and
$Y$ is an $n \times 1$ vector.
We can write the errors as
\begin{align*}
\varepsilon = Y-X \beta
\end{align*}
and we have the following total sum of squared errors,
\begin{align*}
S(\beta) &= \varepsilon^T \varepsilon \\
&= (Y-X\beta)^T (Y-X\beta)
\text{.}
\end{align*}
For the least-squared-error estimate, we want to minimise the sum of squared errors it in terms of $\beta$, so we take the derivative and set it equal to zero.
\begin{align*}
  S(\beta)
  &= Y^T Y -\beta^T X^T Y - Y^T X \beta + \beta^T X^T X \beta \\
  &= Y^T Y - 2 Y^T X \beta + \beta^T X^T X \beta
  & (\text{result is }n \times 1)
  \\
 \frac{\partial}{\partial\beta} S(\beta)
 &= - 2 X^T Y +
    2X^T X \beta
    & (\text{result is }p \times 1)
    \\
 \frac{\partial^2}{\partial\beta \partial\beta^T} S(\beta)
 &= 2
    X^T
    X
    & (\text{result is }p \times p)
    \text{.}
\end{align*}
I will not elaborate on the matrix derivatives of this proof; the easiest way to convince yourself of it is to work with $2 \times 2$ matrices.
Next, we set the first derivative equal to zero to find a critical point of the function,
\begin{align*}
 \frac{\partial}{\partial\beta} S(\beta) &= 0 \\
  2X^T X \beta - 2 X^T Y &= 0 \\
  X^T X \beta &=  X^T Y  \\
   \beta &= (X^T X)^{-1} X^T Y
\end{align*}
giving the required result.
This is a minimum since the second derivative is a positive definite matrix.

\subsubsection{Likelihood}
\index{Likelihood}
To use the likelihood we need to make an assumption on the error.
Assume the individual errors follow a normal distribution,
$\varepsilon_i \sim \text{Normal}(0, \sigma^2)$ for $ i = 1 ,2 , \ldots , n $.
Then the error vector has a multivariate normal distribution, $\varepsilon \sim \text{MultivariateNormal}(\vec{0}, I_n \sigma^2)$,
where $I_n$ is the $n \times n$ identity matrix and $\sigma^2$ is the variance assumed on the errors. Using the same $X$, $Y$, and $\beta$ matrices as above, the linear model
\begin{align*}
  Y = X \beta + \varepsilon
\end{align*}
is equivalent to
\begin{align*}
  Y \sim \text{MultivariateNormal}(X \beta, I_n \sigma^2 )
  \text{.}
\end{align*}
The likelihood for this multivariate normal distribution is
\begin{align*}
\mathcal{L}(\theta;Y) &= f(Y) \\
&=
\frac{1}{\sqrt{(2\pi)^n \determinant{I_n \sigma^2} }}
\exp{
  \left(
  -\frac{1}{2}
   (Y-X\beta)^T  (I_n \sigma^2)^{-1} (Y - X \beta)
  \right)
} \\
&=
\frac{1}{\sqrt{(2\pi)^n  } \sigma^{n}}
\exp{
  \left(
  -\frac{1}{2 \sigma^2}
   (Y-X\beta)^T (Y-X\beta)
  \right)
}
\end{align*}
where $f(Y)$ is the pdf of the multivariate normal distribution.
The vector $\theta = \{ \beta, \sigma \}$ is the set of all parameters we want to estimate.
The maximum likelihood estimate (MLE) is the set of parameters that maximises the likelihood,
\begin{align}
\label{eq:regressiontheory:argmax}
\hat{\theta}_{\text{MLE}}
&=
\argmax_{\theta}
\mathcal{L}(\theta;Y)
\text{.}
\end{align}
Since the logarithm is a strictly increasing function, the likelihood and log likelihood will have a maximum at the same value of $\theta$.
That means we can use either the likelihood or the log likelihood to calculate the MLE, since they will yield the same answer.
The log likelihood is more convenient to work with:
\begin{align*}
\log\mathcal{L}(\theta;Y)
&=
-\log\left(\sqrt{(2\pi)^n} \right)
-\log\sigma^{n}
  -\frac{1}{2 \sigma^2}
   (Y-X\beta)^T (Y-X\beta)
   \text{.}
\end{align*}
As with the least-squares derivation, we need to find a critical point, so we take the derivative with respect to the parameters of interest
\begin{align*}
 \frac{\partial}{\partial\beta}
\log\mathcal{L}(\theta;Y)
 &=
  \frac{1}{2 \sigma^2}
(  2 X^T Y -
    2X^T X \beta )
    \\
 \frac{\partial}{\partial\sigma}
\log\mathcal{L}(\theta;Y)
 &=
-\frac{n}{\sigma}
+  \frac{1}{\sigma^3}
   (Y-X\beta)^T (Y-X\beta)
   \text{,}
\end{align*}
which we set equal to zero
\begin{align*}
0
 &=
  \frac{1}{2 \sigma^2}
(  2 X^T Y -
    2X^T X \beta )
    \\
0
 &=
-\frac{n}{\sigma}
+  \frac{1}{\sigma^3}
   (Y-X\beta)^T (Y-X\beta)
   \text{,}
\end{align*}
and solving this system of equations yields the maximum likelihood estimates
\begin{align*}
\hat{\beta}_{\text{MLE}} &= (X^T X)^{-1} X^T Y
    \\
\hat{\sigma}_{\text{MLE}}
 &=
\sqrt{
\frac{1}{n}
   (Y-X\hat{\beta}_{\text{MLE}})^T (Y-X\hat{\beta}_{\text{MLE}})
   }
   \text{.}
\end{align*}
We can check that these estimates do in fact maximise the log likelihood using the second order derivatives,
\begin{align*}
 \frac{\partial^2}{\partial\beta \partial\beta^T}
 \log\mathcal{L}(\theta;Y)
 &=
 - \frac{1}{ \sigma^2} X^T X
 \\
 \frac{\partial^2}{\partial\sigma^2}
\log\mathcal{L}(\theta;Y)
 &=
\frac{1}{\sigma^2}
\left(
 -\frac{3}{\sigma^2}
   (Y-X\beta)^T (Y-X\beta)
   + n
\right)
   \text{.}
\end{align*}
The first one is
$ -\nicefrac{1}{ \sigma^2}$ multiplied by a positive definite matrix, which will be negative definite.
The second one evaluates to
\[
-2n^2
\left(
  (Y-X\hat{\beta}_{\text{MLE}})^T
  (Y-X\hat{\beta}_{\text{MLE}})
\right)^{-1}
\]
when $\sigma = \hat{\sigma}_{\text{MLE}}$, which is also negative quantity multiplied by a positive definite matrix.


Notice that the $\beta$ estimates are the same for least-squares estimation and maximum likelihood estimation.
This is only due to the assumption of the normal distribution and it will not hold in general for GLMs.
In addition, there will seldom be an analytical solution like the one presented.
For most GLMs we have to maximise the likelihood---or rather, the log likelihood---numerically, using an iterative approach like Newton's method or similar.


\subsection{Simple linear regression}
\index{Regression theory!Simple linear}

The simple linear regression is the special case of the linear regression with only one covariate
\[
  y = \alpha + x \beta
  \text{,}
\]
which is just a straight line fit.
Interviewers like this model for its many aesthetically pleasing theoretical properties.
We will describe a few of them here, beginning with parameter estimation.
For $n$ pairs of $(x_i, y_i)$,
\[
  y_i = \alpha + \beta x_i + \varepsilon_i
\]
and we will minimise the sum of squared errors,
\[
\sum_{i=1}^{n}{  \varepsilon_i^2 }
  =
\sum_{i=1}^{n}{ (y_i -  \alpha + \beta x_i)^2 }
\text{,}
\]
similar as before, but without matrices.
The derivation is left as an exercise to the reader.
The least-squares estimators are:
\begin{align}
  \hat{\alpha} &= \bar{y} - \hat\beta \bar{x} \nonumber \\
  \hat{\beta} &=
  \frac
  {\sum_{i=1}^{n}{ (y_i - \bar{y}) (x_i - \bar{x}) }}
  {\sum_{i=1}^{n}{ (x_i - \bar{x})^2 }} \nonumber \\
  &=
  \frac
  {\Cov(x, y)}
  {\Var(x)} \label{eq:simple_linear_regression:var} \\
  &= \rho_{xy} \frac{s_y}{s_x} \label{eq:simple_linear_regression:corr}
  \text{.}
\end{align}
Here, we use
\begin{align*}
\bar{y} &= \frac{1}{n}\sum_{i=1}^{n}{y_i} \\
\bar{x} &= \frac{1}{n}\sum_{i=1}^{n}{x_i}
\end{align*}
and $\Var$ and $\Cov$ are the variance and covariance, respectively.
Likewise,
$\rho_{xy}$,
$s_y$, and
$s_x$ are the sample correlation coefficient and sample standard deviations.
The last two lines of the equation,
(\ref{eq:simple_linear_regression:var})
and
(\ref{eq:simple_linear_regression:corr}),
are of utmost importance.
Learn them by heart, as they make many questions about simple linear regression trivial.
In fact, if you do not know them by heart you will struggle to answer these questions---questions your interviewer will expect a first-year student to know.
For example, ``when is the slope of a straight-line fit through a set of points $(x_i,y_i)$ equal to the correlation between $x$ and $y$?''

The final important result is that, for a simple linear regression, the $R^2$ value (which measures the proportion of variance in $y_i$ explained by the variance in $x_i$) is equal to the sample correlation coefficient squared,
\[
  R^2 = \rho_{yx}^2
  \text{.}
\]


\subsection{Logistic regression}
\index{Regression theory!Logistic}
Both the logistic regression, as well as the linear regression, are GLMs as introduced by \citet{GLM}, but users might spend years using regressions without ever encountering this term.
The techniques have different names in different fields, a phenomena that has been exacerbated by the recent ``rediscovery'' of linear regression and logistic regression in Machine Learning.
It is prudent to know some specialist theory and important results (nay, tricks) about each of the three regressions discussed in this section, using the terminology that pertains to finance.

I will present the logistic regression from the perspective of a GLM.
The assumptions are the same as that of the linear regression, except that we are assuming linearity on the logit scale.
Let our variate of interest $y$ be a binary variable taking values of $0$ and $1$, then
\begin{align*}
y &\sim \text{Bernoulli}(p) \\
\text{logit}(p) &= X\beta  \quad (\text{logit link function})\\
\text{or}& \\
p &= \text{logistic}(X\beta)
\text{.}
\end{align*}
Hence the name logistic regression.
In fact, the only difference between the logistic and the probit regression is the link function.
For the probit regression we have
\begin{align*}
y &\sim \text{Bernoulli}(p) \\
\text{Probit}(p) &= X\beta  \quad (\text{Probit link function})\\
\text{or}& \\
p &= \Phi(X\beta)
\text{,}
\end{align*}
where $\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution, and   $\text{Probit}(x)$ is the quantile function (or inverse CDF).\footnote{Notice that the naming convention isn't consistent, mostly due to historical reasons beyond the scope of this text.}
The R-square measure as defined for the linear regression doesn't exist for the logistic regression.
There are a few measures that act as pseudo R-squared, but I won't discuss them here, revert to the Wikipedia page for details.%
\footnote{\url{https://en.wikipedia.org/wiki/Logistic_regression\#Pseudo-R2s}}

\subsubsection{Significance tests}
Where the linear regression uses the t-statistic for significance testing, the logistic regression has the Wald-statistic. Asymptotically, it has a chi-squared distribution:
\[
W_{\hat\beta} =
\frac{(\hat\beta - \beta_0)^2}{ \text{stdev}( \tilde\beta) }
\sim \mathcal{X}^2
\]
where $\beta_0$ is a hypothesised value, as with the linear regression.



\subsubsection{Parameter estimation}

Similar to the logistic regression, we have the two schools of thought about parameter estimation.
\begin{description}
  \item[Least squares and related techniques.]
    Unlike the linear regression, we cannot get a closed-form solution and thus we usually have to use some sort of iterative method. Iteratively reweighted least squares is usually employed.
  \item[Techniques based on the likelihood.]
  A frequentist would need to maximise the likelihood using a numerical technique, whereas a Bayesian would evaluate the posterior distribution numerically, or sample from it using MCMC.
\end{description}



\section{Soft interview}

\subsection{Questions from them}

For examples of soft interview questions (with suggested answers) see \citet{JoshiQA}.
Be ready to give some answers to the following questions.
\begin{itemize}
  \item Walk me through your CV.
  This is the most common one and is usually asked at the start of a phone interview.
  Rehearse an answer and try to keep it shorter than three minutes.
  It is vital to prepare a response and to tell it like a story.
  It looks bad if you can't articulate the contents on your CV and that then sets the tone for the rest of the interview.
  \item Why are you looking for a new job?
  I once had a phone interview where, upon answering, the guy immediately blurted ``Hi OK tell me why do you want to leave your current job for one in finance?''
  He didn't even pause to tell me his name.\footnote{Never mind that it was their recruiter who reached out to me.}
  You should have your answer to this pointed question so well rehearsed that it sounds completely natural.
  \item Why do you want to work in finance?
  \item Why do you want to work for this firm and not our competitor?
\end{itemize}
%
Try not to start your sentences with ``So\ldots''.
It takes discipline to avoid, but it makes you sound more confident.
Also try and keep the number of filler words such as ``like'', ``uhm'', and ``you know'' to a minimum.
You don't have to completely remove them to the point of sounding unnatural, but it is important to pay attention to how often you use them---especially under pressure.
This is most relevant to phone interviews, where your first impression is wholly determined by your eloquence.
Rehearse some answers to the questions above out loud.
Keep in mind that there is likely to be a difference between your accent and that of the interviewer and adjust your tempo and enunciation accordingly.

\subsection{Questions to them}

At the end of every interview, interviewers are likely to give you time for questions.
Have something to ask them.
Also, prepare some in advance since you will be too fatigued at the end of the interview to think fast.
If there are any obvious questions that come to light during the interviews, ask them.
I've seen interviewing guides online suggest questions like
``What is your greatest challenge over the coming months, and how will I be able to contribute to a solution.''
In my opinion this question feels rehearsed, and I don't recommend using it.
It's been suggested so often, however, that I've started thinking the problem lies with me.
Try it for yourself and see.
There are perhaps more interesting questions you can ask, which are also relevant to the job.
When you ask these questions, guard against expressing a preference for any of the systems. Rather, ask it out of interest; you want to know what the job will be like so you can prepare yourself.
\begin{itemize}
  \item What operating systems does the team use?
  What is the proportions of Windows, Apple, GNU/Linux?
  \item  What does the team use for technical documentation? LaTeX/Wiki/Word/Nothing? (Don't say ``knowledge management'', you aren't interviewing to become a management consultant.)
  \item  What software do you use for version control? Svn, Git, Mercurial, Team Foundation Server, or appending
  \verb+V1+,
  \verb+V2+,
  \verb+V3+
  to foldernames?
  \item  What programming languages does the team use? R, Python, C++, Julia, Matlab?
  For data analysis? For scripting? For production?
  \item  Are the team Bayesians, frequentists, or whatever gets the job done?
  \item  What are the backgrounds of most of the team members? What will someone with my background contribute?
  \item  Are modellers expected to write the code to implement their models, or is this handled by developers?
  \item  How are datasets accesses? SQL (Postgres, MySQL, MSSQL, sqlite), Hadoop, csvs, Excel, shared folders, or do people throw around USB sticks?
  \item  Are there formal mentor/mentee arrangements, or does it happen informally? (Or not at all?)
  \item  Is there a formal process for requesting new software libraries to be installed? This is more R and Python specific, but it is important to ask if you expect to work with the latest technologies.
\end{itemize}
Even if you have asked these questions to some of the team members, ask them again and see whether you get the same answers.







\clearpage
\appendix

\phantomsection
\addcontentsline{toc}{section}{Appendix}
\section*{Appendix}

% This line sets the tocdepth from here on forward
% It means we hide subsections
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}

\section{Tricks}
\label{ap:tricks}
\index{Tricks}
I include some tricks, which I often found useful while being brainteased.
Tricks and teasing go together.

\subsection{Start with simplest case}
\index{Tricks!Start with simplest case|textbf}
Start with the simplest case, and generalise thereafter.
This often involves considering the case where $n=1$, then $2,3,4$, and so forth.
Though it feels simple it is not devoid of rigour, since it is often obligatory preparation for a proof by induction.
It is the most important trick of all.
During an interview, a never-before-seen brainteaser usually catches you by surprise and affects your ability to think clearly---especially since you have only 10 or 20 minutes to make an attempt.
When this happens the $n=1$ trick becomes a saviour.
Even if you don't manage to solve the problem you'll have shown the interviewer that you approach a difficult problem by breaking it down to its integral parts.
The best example of this appears in question
\ref{q:simplerandomwalkhittingprobability}.
Find further examples of this trick in questions
\ref{q:arraymissingnumber:a},
\ref{q:criminalsinfield},
and
\ref{q:bagnsockstwored}.

\subsection{Skip integration by drawing the unit square}
\index{Tricks!Unit square integration|textbf}
If a question involves two uniform distributions, chances are you can skip any integration by drawing a unit square and calculating areas on it.
More often than not this is also what the interviewer wants.
For examples of this, see the Stick Breaking problem in section \ref{subsec:stickbreaking} and question \ref{q:romeojuliet}.
In both these examples the integration would be cumbersome, but calculating the areas of triangles on the unit square is simple (if you correctly identify the areas of interest, that is).
The same idea is used in question \ref{q:rolltwodice} for a discreet sum.

\subsection{Recursion}
This is a trick I never encountered before I started interviewing.
We needed it in section \ref{q:airforceone} with the Air Force One question, as well as question \ref{q:simplerandomwalkhittingprobability}.
The idea is to express the answer you want as $P(A_n)$ or $p_n$ and then you express $p_n$ in terms of $p_{n-1}$.
Then you can use recursion until you can solve for the quantity you want.
The same trick is sometimes used to calculate expectations of interest (using the law of total expectation).
For a good example, see the question called ``Biased coins'' from \citet{WilmottFAQ}, and also the one called ``Two heads'' from the same book, which I reproduce here as a minimal-working example showing the method.

\begin{quote}
 When flipping an unbiased coin, how long do you have to wait on average before you get two heads in a row?
\end{quote}
I don't like the notation from Wilmott's answer since it is unclear whether his $N_n$ denotes an expectation or a probability, so I will amend it.
Let $N_n$ be a random variable representing the number of coin flips we have to do to get $n$ heads in a row.
It satisfies the recursion relationship:
\begin{align*}
  N_n &=
  \overbrace{ \frac{1}{2} (N_{n-1} + 1)}^{n\text{th throw is head}}
  +
  \overbrace{\frac{1}{2} (N_{n-1} + 1 + N_n)}^{n\text{th throw is tail}}
  \\
  \frac{1}{2}  N_n &=
   N_{n-1} + 1
  \\
  N_n &=
  2 N_{n-1} + 2
  \text{.}
\end{align*}
Wilmott solves for $N_n$, but that doesn't make sense if it is a random number.
Rather, we should solve for the expected value of $N_n$,
\begin{align*}
  E(N_n) &= 2 E(N_{n-1}) + 2
  \text{.}
\end{align*}
The expected number of throws to get one head is $2$, so
$E(N_1) = 2$ (use the negative binomial distribution if you need to prove this),
\begin{align*}
  E(N_2) &= 2(2) + 2 = 6
  \text{.}
\end{align*}
This has the general solution of
\begin{align*}
  E(N_n) = 2^{n+1} - 2
\end{align*}
which you can prove with mathematical induction.
Questions that require martingale theory can often also be solved using the recursion technique, albeit with substantially more algebra (like question \ref{q:simplerandomwalkhittingprobability}).

\subsection{Central Limit Theorem}
\index{Tricks!Central Limit Theorem|textbf}
Here the trick lies in identifying when a question is actually about the Central Limit Theorem.
I will not review the theory here, but make sure you understand the intuitive consequences:
If we sum a large number of random variables, the distribution of the sum approaches the normal distribution.\footnote{How many are required? Statisticians agree that 30 is large enough to be treated as infinity.}
For instance, here is a trick question.
\begin{quote}
You need 100 meters of wood.
One factory makes 100 meter pieces and another factory makes 1 meter pieces.
Both factories cite a 20\% error on their output.
Does it matter which factory you use?
\end{quote}
Note that the expected length of the end product is the same, and so is the variance.
The difference is the distribution.
You don't know what the error distribution of the $100m$ piece will be (the question doesn't tell us), but the sum of $1m$ pieces will have a distribution that is near normal.
If you worry about large outliers you probably prefer the normal distribution.
This is why insurers write millions of small policies rather than one big one.
Under some regularity conditions, their total loss will follow a normal distribution.
See question \ref{q:coin100flipsgamble} for the normal approximation to the binomial distribution, which is the same as the Central Limit theorem applied to a sum of Bernoulli random variables.


\subsection{Normal distribution probabilities in your head}
\index{Tricks!Normal probabilities in your head|textbf}
A useful property about the normal distribution is that approximately 95\% of its mass lies within two standard deviations of the mean.
Some questions, like question \ref{q:coin100flipsgamble}, use this by having their quantities of interest lie two standard deviations from the mean.
The actual probability at this point is
\[
\Phi(2)
=  0.97725
\text{.}
\]
That is, if we have
$Z \sim \text{Normal}(0,1)$
then
$P(-2 <  Z \leq 2) = \Phi(2)- \Phi(-2) = 0.9545$.


\subsection{Law of Total Expectation}
\index{Tricks!Law of Total Expectation|textbf}
Many brainteasers are interested in the expectation of some event, or the expected number of trials.
Thus, it is useful to know the Law of total Expectation:
If $ A_1, A_2, A_3, \ldots, A_n$ are mutually disjoint and exhaustive, then
\[
  \E(Z) = \sum_{i=1}^{n}{
    \E(Z| A_i)P(A_i)
  }
  \text{.}
\]
We encountered this in question \ref{q:bivariatenormalmax}, which is a good example of its use.


\section{Code}
\subsection{Expectation of maximum of the bivariate normal distribution}
\label{ap:mvtnormproof}
Here is R code for a pseudo proof-by-simulation of question \ref{q:bivariatenormalmax}.

\inputminted{r}{./plots/mvtnorm/simtest.R}


\subsection{Why we need virtual functions}
\label{ap:virtualfunctions}
I produce my own take on the answer found at \\
\url{https://stackoverflow.com/questions/2391679/ddg#2392656}\\
Say you are working for the tax man, and you want to create a function that assigns the right tax rate to cars.
If you don't know the type of car, you will assume it burns fuel.
So let's make a class for generic cars and one for electric cars.
\inputminted{cpp}{./plots/virtualfunction/WithoutVirtual.cpp}
When we run this program, the output is:
\VerbatimInput{./plots/virtualfunction/WithoutVirtual.txt}
Oh no! The last line is wrong, we have accidentally treated the Tesla as a general car.
We would have to overload the \verb+queryEmssionsForTax+ function to take a
class of type
\verb+ElectricCar+.
But we don't want to burden the author of the
\verb+ElectricCar+
class with a list of external functions to maintain.
The solution is to use a virtual function.
Below, the only changes we make is to add
\verb+virtual+
to the function in the
\verb+Car+
class and
\verb+override+
to the function in the child class.
\inputminted{cpp}{./plots/virtualfunction/WithVirtual.cpp}
Now the output becomes:
\VerbatimInput{./plots/virtualfunction/WithVirtual.txt}
Success.

\subsection{SQL query practise script}
\label{ap:sqlite}
This script loads the tables and gives you the ability to test SQL queries on them.
I use the \verb+pandas+ and \verb+sqlite+ libraries, but you probably only need \verb+sqlite3+.
I use Python since it is easy to install on most operating systems and it comes with sqlite functionality as standard.
\inputminted{python}{./plots/sql/sql.py}

\clearpage
\section{Crib sheet}
\label{ap:cribsheet}
Print this sheet and keep it with you during phone interviews, but make sure you are able to reproduce it wholly from memory at some point in your life.
Actual familiarity with the concepts is better than rote, but a combination of the two is required for good interview performance.


\begin{multicols}{2}

\subsection*{Miscellaneous}
ARMA(p, q):
\[
 Y_t = \alpha_0 +
  \sum_{i=1}^{p}{ \alpha_i Y_{t-i} }
  +
  \sum_{i=1}^{q}{ \beta_i \varepsilon_{t-i} }
  +
  \varepsilon_{t}
\]
\noindent
GARCH(p, q):
\begin{align*}
 Y_t &= \sigma_t Z_t  \quad \text{ where } Z_t \sim N(0,1) \\
 \sigma^2_{t}
 &=
 \alpha_0 +
  \sum_{i=1}^{p}{ \alpha_i Y^2_{t-i} }
  +
  \sum_{i=1}^{q}{ \beta_i \sigma^2_{t-i} }
\end{align*}


\begin{align*}
P(A_i|B)
&= \frac {P(B|A_i)P(A_i)} {P(B)} \\
&= \frac {P(B|A_i)P(A_i)} { \sum_{j} P(B|A_j)P(A_j) }
\end{align*}

\begin{align*}
{}_nC_{r} =
\binom{n}{k} &= \frac{n!}{ (n-k)! k! }
\end{align*}

\begin{align*}
  \sum_{k=0}^{n-1} {ar^k}    &= a \left(\frac{1-r^n}{1-r}\right) \\
  \sum_{k=0}^{\infty} {ar^k} &= \frac{a}{1-r} \text{ for } |r|<1
\end{align*}

\subsection*{Regression assumptions}

\begin{itemize}[itemsep=-2pt, topsep=2pt, partopsep=0pt]
  \small
  \item The covariates are observed without error (Weak exogeneity)
  \item Linearity
  \item Constant variance (Homoscedasticity)
  \item Independence of errors
  \item No (or very little) multicollinearity
  \item Statistical distribution (Optional)
\end{itemize}

\subsection*{Linear regression}
\[
    \hat{\beta} = (X^T X)^{-1}X^T y
\]

\[
t_{\hat\beta}  =  \frac{\hat\beta - \beta_0}{ \text{stdev}( \tilde\beta) } \sim t(v=n-p)
\]

\subsection*{Simple linear regression}
\begin{align*}
  \hat{\alpha} &= \bar{y} - \hat\beta \bar{x}  \\
  \hat{\beta} &=
  \frac
  {\sum_{i=1}^{n}{ (y_i - \bar{y}) (x_i - \bar{x}) }}
  {\sum_{i=1}^{n}{ (x_i - \bar{x})^2 }}  \\
  &=
  \frac
  {\Cov(x, y)}
  {\Var(x)} \\
  &= \rho_{xy}
\end{align*}

\[
  R^2 = \rho_{yx}^2
\]

\subsection*{Logistic regression}
\[
W_{\hat\beta}
=  \frac{(\hat\beta - \beta_0)^2}{ \text{stdev}( \tilde\beta) }
\sim \mathcal{X}^2
\]


\subsection*{Calculus}
The following are important for most brainteasers:
\begin{itemize}
  \item
Product rule of differentiation:
\[
  \frac{d}{dx}
  g(x)f(x) =
  g'(x)f(x) +
  g(x)f'(x)
\]

  \item
Integration by parts:
\[
\int_{a}^{b}{ u dv }
=
uv\Big\vert_{x=a}^{x=b} - \int_{a}^{b}{ v du }
\]
  \item
Integration and differentiation rules involving $e^{x}$ and $\ln(x)$.
\end{itemize}


\end{multicols}
\clearpage




\phantomsection
\addcontentsline{toc}{section}{References}

\bibliography{books.bib}
%\bibliographystyle{plainnat}
\bibliographystyle{chicago}

\phantomsection
\addcontentsline{toc}{section}{Index}
\printindex

\end{document}

